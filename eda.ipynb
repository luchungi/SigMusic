{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'theorytab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{folder}/all_melodies.pkl', 'rb') as f:\n",
    "    songs = pickle.load(f)\n",
    "dfs = [item[0] for item in songs]\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = f'./data/{folder}/'\n",
    "# songs = get_dfs_from_midi(path, min_notes=30, min_gap=0., melody_only=True)\n",
    "# dfs = [item[0] for item in songs]\n",
    "# print(len(dfs), len(songs))\n",
    "\n",
    "# with open(f'./data/dataframes/{folder}/all_melodies.pkl', 'wb') as f:\n",
    "#     pickle.dump(songs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = np.random.randint(len(dfs))\n",
    "print(f'Artist, title, segment of song: {songs[sample_idx][1:4]}')\n",
    "print(f'Key of song: {songs[sample_idx][4]}')\n",
    "print(f'Time signature: {songs[sample_idx][5]}')\n",
    "print(f'Beats in song: \\n {songs[sample_idx][6]}')\n",
    "midi_data = df_to_midi(songs[sample_idx][0])\n",
    "fs = 44100\n",
    "audio_data = midi_data.fluidsynth(fs=fs)\n",
    "ipd.Audio(audio_data, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keys_per_song = [len(item[4]) for item in songs]\n",
    "plt.hist(n_keys_per_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage of notes within Major scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_to_note = {0: 'C', 1: 'C#', 2: 'D', 3: 'D#', 4: 'E', 5: 'F', 6: 'F#', 7: 'G', 8: 'G#', 9: 'A', 10: 'A#', 11: 'B'}\n",
    "key_notes = []\n",
    "for i in range(12):\n",
    "    key_notes.append(get_notes_from_major_scale(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check percentage of notes in a key\n",
    "key = 5\n",
    "\n",
    "dfs_key = [item[0] for item in songs if item[4][0][0] == key]\n",
    "key_percentages = {0: [], 1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: []}\n",
    "for df in dfs_key:\n",
    "    for i in range(len(key_notes)):\n",
    "        percentage = len([note for note in df['Pitch'].values if note in key_notes[i]]) / len(df['Pitch'].values)\n",
    "        key_percentages[i].append(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 3, figsize=(15, 5))\n",
    "print(f'Key: {key_to_note[key]}')\n",
    "for i in range(len(key_notes)):\n",
    "    print(f'Mean percentage of notes in key {key_to_note[i]}: {np.mean(key_percentages[i])}')\n",
    "    ax[i//3, i%3].hist(key_percentages[i], bins=100)\n",
    "    ax[i//3, i%3].set_title(f'{key_to_note[i]} major scale')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out songs which have accidentals i.e. notes not in the key\n",
    "songs_within_key = []\n",
    "for i in range(len(songs)):\n",
    "    key = songs[i][4][0][0]\n",
    "    key_notes = get_notes_from_major_scale(key)\n",
    "    notes = songs[i][0]['Pitch'].values\n",
    "    if all([note in key_notes for note in notes]):\n",
    "        songs_within_key.append(songs[i])\n",
    "len(songs_within_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{folder}/all_melodies_notes_within_key.pkl', 'wb') as f:\n",
    "    pickle.dump(songs_within_key, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = songs_within_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_signatures_per_song = [len(item[5]) for item in songs]\n",
    "plt.hist(n_signatures_per_song, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beats_per_bar = [item[5][0][0] for item in songs]\n",
    "plt.hist(beats_per_bar, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_value = [item[5][0][1] for item in songs]\n",
    "plt.hist(note_value, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all beats are multiples of the first beat\n",
    "beats = [item[6] for item in songs]\n",
    "beats = [item / item[1] for item in beats]\n",
    "beats = np.concatenate(beats)\n",
    "beats += 1\n",
    "beats = beats % beats == 0\n",
    "beats.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songs_beats_aligned = []\n",
    "# for song in songs:\n",
    "#     seconds_per_beat = song[6][1]\n",
    "#     df = song[0]\n",
    "#     df.loc[:, ['Start', 'End']] /= seconds_per_beat\n",
    "#     df.loc[:, ['Start', 'End']] *= 0.5\n",
    "#     songs_beats_aligned.append((df, song[1], song[2], song[3][:-4], song[4][0][0])) # df, artist, song-title, segment, key\n",
    "#     # print(seconds_per_beat)\n",
    "#     # print(songs_beats_aligned)\n",
    "# with open(f'./data/dataframes/{folder}/all_melodies_within_key_beats_aligned.pkl', 'wb') as f:\n",
    "#     pickle.dump(songs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{folder}/all_melodies_beats_aligned.pkl', 'rb') as f:\n",
    "    songs = pickle.load(f)\n",
    "len(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 30\n",
    "min_unique_notes = 5\n",
    "songs = [item for item in songs if len(item[0]) > min_length]\n",
    "songs = [item for item in songs if item[0]['Pitch'].nunique() > min_unique_notes]\n",
    "lens = [len(item[0]) for item in songs]\n",
    "print('Number of songs:', len(songs))\n",
    "print('Max length:', max(lens))\n",
    "print('Min length:', min(lens))\n",
    "print('Mean length:', np.mean(lens))\n",
    "sorted_lens = sorted(lens)\n",
    "plt.bar(range(len(sorted_lens)), sorted_lens, width=1.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_idx = np.argpartition(lens, -4)[-4:]\n",
    "print(largest_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 756\n",
    "print(songs[sample_idx][1:4])\n",
    "midi_data = df_to_midi(songs[sample_idx][0])\n",
    "fs = 44100\n",
    "audio_data = midi_data.fluidsynth(fs=fs)\n",
    "ipd.Audio(audio_data, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./data/dataframes/{folder}/all_melodies_within_key_beats_aligned_min_5_unique.pkl', 'wb') as f:\n",
    "#     pickle.dump(songs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{folder}/melodies_beats_min_5_unique.pkl', 'rb') as f:\n",
    "    songs = pickle.load(f)\n",
    "len(songs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are more than 1 note at the same time\n",
    "for i, song in enumerate(songs):\n",
    "    if song[0]['Start'].duplicated().any():\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration of melodies\n",
    "durations = []\n",
    "for item in songs:\n",
    "    durations.append(item[0]['End'].max())\n",
    "plt.hist(durations, bins=100);\n",
    "min(durations), max(durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitch range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range([item[0] for item in songs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs2 = trim_by_range(songs, min_range=5, max_range=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range([item[0] for item in songs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'./data/dataframes/{folder}/all_melodies_within_key_beats_aligned_min_5_unique_max_range_21.pkl', 'wb') as f:\n",
    "#     pickle.dump(songs2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure gap between start of next note and end of previous note\n",
    "neg_gaps = []\n",
    "pos_gaps = []\n",
    "for df in dfs:\n",
    "    gap = df['Start'].iloc[1:].values - df['End'].iloc[:-1].values\n",
    "    if gap.min() < 0:\n",
    "        neg_gaps.append(df)\n",
    "    if gap.max() > 0:\n",
    "        pos_gaps.append(df)\n",
    "print(len(neg_gaps), len(pos_gaps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duration + Pitch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs4 = note_duration_transform(dfs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs5, max_pitch = pitch_translation(dfs4)\n",
    "max_pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_range(dfs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{key}/dfs_note_dur_offset_{int(max_pitch)}.pkl', 'wb') as f:\n",
    "    pickle.dump(dfs5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/dataframes/{key}/dfs_note_dur_offset_{int(max_pitch)}.pkl', 'rb') as f:\n",
    "    dfs5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs5[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NoteDurationDataset(dfs5, sample_len=20, scale=1., stride=10)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=0, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
